# -*- coding: utf-8 -*-
"""final_lstm_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mq2xN5jaOgcT-BlW-fh04CjTQnhuZ91c
"""



import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

dns = pd.read_csv('DoS_DNS.csv')
ldap = pd.read_csv('DoS_LDAP.csv')
mssql = pd.read_csv('DoS_MSSQL.csv')
netbios = pd.read_csv('DoS_NETBIOS.csv')
ntp = pd.read_csv('DoS_NTP.csv')
snmp = pd.read_csv('DoS_SNMP.csv')
ssdp = pd.read_csv('DoS_SSDP.csv')
syn = pd.read_csv('DoS_SYN.csv')
tftp = pd.read_csv('DoS_TFTP.csv')
udp = pd.read_csv('DoS_UDP.csv')
udplag = pd.read_csv('DoS_UDPLAG.csv')

data = pd.concat([dns, ldap, mssql, netbios, ntp, snmp, ssdp, udp, syn, udplag], ignore_index = True)

data.shape

data[' Label']

# Drop Unnamed:0, Unnamed:0.1 columns 
data = data.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1)

data.columns

data_real = data.replace(np.inf, np.nan)

data_real.isnull().sum().sum()

data_df = data_real.dropna(axis=0)

data_df.isnull().sum().sum()

data_df

# data_df.to_csv('data_final.csv', index = False)
# from google.colab import files
# files.download('data_final.csv')

data_X = data_df.drop([' Label', 'SimillarHTTP'], axis = 1)

data_X.columns

data_X.shape

data_y = data_df[' Label']

data_y.shape

data_df.isnull().sum().sum()

data_y.unique()

data_X

"""### Label Encoding for the Dataset """

le = LabelEncoder()
data_y_trans = le.fit_transform(data_y)

le_fid = LabelEncoder()
le_fid.fit(data_X['Flow ID'])
data_X['Flow ID'] = le_fid.fit_transform(data_X['Flow ID'])
le_SIP = LabelEncoder()
le_SIP.fit(data_X[' Source IP'])
data_X[' Source IP'] = le_SIP.fit_transform(data_X[' Source IP'])
le_DIP = LabelEncoder()
le_DIP.fit(data_X[' Destination IP'])
data_X[' Destination IP'] = le_DIP.fit_transform(data_X[' Destination IP'])
le_timestamp = LabelEncoder()
le_timestamp.fit(data_X[' Timestamp'])
data_X[' Timestamp'] = le_timestamp.fit_transform(data_X[' Timestamp'])

"""## Feature Selection """

from sklearn.feature_selection import chi2 
from sklearn.feature_selection import SelectKBest 
from sklearn.ensemble import ExtraTreesClassifier

#selecting 20 best features
# select_best= SelectKBest(chi2, k=20)
# X_feat_20 = select_best.fit_transform(data_X, data_y_trans)
# X_feat_20.shape

model = ExtraTreesClassifier(random_state=42)
#creates an Extra Trees classifier model with a random state of 42. 
#The random state is used to initialize the random number generator, which can help to improve the reproducibility of the results.
model.fit(data_X, data_y_trans)

model.feature_importances_
#
#The model.feature_importances_ attribute of an LSTM model is a NumPy array that stores the importance of each feature in the model. 
#The importance of a feature is a measure of how much the feature contributes to the model's predictions

feature_importance_std = pd.Series(model.feature_importances_, index=data_X.columns)
feature_importance_std.nlargest(20).plot(kind='bar', title='Standardised Dataset Feature Selection using ExtraTreesClassifier')

data_X.shape

data_new_20features_X = data_X[[' Timestamp', ' Source Port', ' Min Packet Length', ' Fwd Packet Length Min', 'Flow ID', ' Packet Length Mean', ' Fwd Packet Length Max', ' Average Packet Size', ' ACK Flag Count', ' Avg Fwd Segment Size', ' Fwd Packet Length Mean', 'Flow Bytes/s', ' Max Packet Length', ' Protocol', 'Fwd Packets/s', ' Flow Packets/s', 'Total Length of Fwd Packets', ' Subflow Fwd Bytes', ' Destination Port', ' act_data_pkt_fwd']]

data_new_20features_X

"""### Train Test Split Normal dataset 84 Features """

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_X, data_y_trans, test_size = 0.30, random_state = 42)

X_train.shape

X_test.shape

"""### Standardization of the 84 Feature Dataset """

from sklearn.preprocessing import StandardScaler 
ss = StandardScaler()
X_train_std = ss.fit_transform(X_train)
X_test_std = ss.fit_transform(X_test)

"""# **Spliting dataset**"""

from sklearn.model_selection import train_test_split
X_train_20, X_test_20, y_train_20, y_test_20 = train_test_split(data_new_20features_X, data_y_trans, test_size = 0.30, random_state = 42)

"""### Standardization of the Feature Dataset """

from sklearn.preprocessing import StandardScaler 
ss_20 = StandardScaler()
X_train_std_20 = ss_20.fit_transform(X_train_20)
X_test_std_20 = ss_20.fit_transform(X_test_20)

X_train_std_20.shape

y_train_20.shape

X_test_std_20.shape

y_test_20.shape

"""###  LSTM model"""

import keras
import keras.utils
from keras import utils as np_utils
from keras.utils import to_categorical

y_train_lstm_20 = np.array(y_train_20)
y_test_lstm_20 = np.array(y_test_20)

y_train_onehot_lstm = to_categorical(y_train_lstm_20,13)
y_test_one_hot_lstm = to_categorical(y_test_lstm_20,13)

X_train_lstm_20 = np.array(X_train_std_20)
X_test_lstm_20 = np.array(X_test_std_20)

X_test_std_20

X_train_lstm_20.shape[0]

X_train_lstm_reshape = np.reshape(X_train_std_20, (X_train_lstm_20.shape[0], 1,  X_train_lstm_20.shape[1]))
X_test_lstm_reshape = np.reshape(X_test_std_20, (X_test_lstm_20.shape[0], 1, X_test_lstm_20.shape[1]))

from keras.models import Sequential
from tensorflow.python.keras.layers import Dense, LSTM
from tensorflow.python.keras.models import Sequential
from keras.layers import Activation, Dense
from keras.layers import Dense, BatchNormalization, Dropout, LSTM
from keras.optimizers import Adam
from tensorflow.keras import regularizers
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score
from keras import callbacks
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = callbacks.EarlyStopping(
    min_delta=0.001,
    patience=20, 
    restore_best_weights=True,
)
batch_size = 256

# Initialize the network
model_LSTM = Sequential()
model_LSTM.add(LSTM(8,input_dim=20, return_sequences=True)) 
model_LSTM.add(Dropout(0.1))
model_LSTM.add(LSTM(8,input_dim=20, return_sequences=False))
model_LSTM.add(Dropout(0.1))
model_LSTM.add(Dense(13))
model_LSTM.add(Activation('softmax'))

monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',
      restore_best_weights=True)

model_LSTM.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model_LSTM.fit(X_train_lstm_reshape, y_train_onehot_lstm, validation_data=(X_test_lstm_reshape, y_test_one_hot_lstm),batch_size=batch_size, epochs=10,callbacks=[monitor])

y_perd_lstm = model_LSTM.predict_classes(X_test_lstm_reshape)

print("Classification Report for LSTM: \n", classification_report(le.inverse_transform(y_test_lstm_20), le.inverse_transform(y_perd_lstm)))

lstm_conf_mat = confusion_matrix(y_test_lstm_20, y_perd_lstm)
print("LSTM Confusion: \n", lstm_conf_mat)

acc_score_lstm = accuracy_score(y_test_lstm_20, y_perd_lstm)
print("Accuracy Score for MLP: \n", acc_score_lstm*100)